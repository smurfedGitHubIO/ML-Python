{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c8b44a-82af-46d6-ba06-974da4fb76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd8cee7-a4bb-41d7-a1cf-b3156faed53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]),\n",
       " tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12, dtype=torch.float32)\n",
    "B = A.clone() # copy A to B\n",
    "A, A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115734e3-32a0-400d-980c-aa60df6b5001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   1.,   4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100., 121.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B # A*B is element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177466f5-96ac-4b2a-b777-143a5e113435",
   "metadata": {},
   "source": [
    "What if magkaiba sila ng size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa44dbd-b56c-49ba-85b3-c60d72176d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m12\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      2\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mA\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mB\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "A = torch.arange(12, dtype=torch.float32)\n",
    "B = torch.arange(2, dtype=torch.float32)\n",
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae2f6d-7a0c-4d4c-a230-385dc2007206",
   "metadata": {},
   "source": [
    "Seems like there's no broadcasting applied on Hadamard products. Note that broadcasting is sometimes a bad idea due to inefficient memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897221a-2e4d-4342-a2db-916e6fa8c3a7",
   "metadata": {},
   "source": [
    "Note on views and copies: Views can be made through basic indexing (bale kung may ginawa kang modifications dun sa first, makikita yung changes dun sa view) and copies can be made through advanced indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc3d6a-0f3a-4f9b-8464-08e1783dbce6",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd4dc7f-e566-428b-b0b7-63b2869812fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3]\n",
    "            ],\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3]\n",
    "            ],\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3]\n",
    "            ],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faeacec-2440-40db-8bfb-f0805e60eb5a",
   "metadata": {},
   "source": [
    "That clears it. `sum` method reduction is done through the rows and columns of each inner matrices/vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1254a-636c-4221-98b7-46fa9ff74475",
   "metadata": {},
   "source": [
    "There's another method called `cumsum` that takes the cumulative sum of the tensor on its respective dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacefa9-ad82-428c-a2a2-ff507c4ca321",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1c6e1-13ab-4b1a-a5f1-9d4d3941311f",
   "metadata": {},
   "source": [
    "1. Prove that the transpose of the transpose of a matrix is the matrix itself "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cc0bc-7519-4e08-b97a-c77a45fe9ba5",
   "metadata": {},
   "source": [
    "2. Given two matrices A\n",
    " andB\r\n",
    ", show that sum and transposition comme.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdf1c331-1beb-438c-8a17-27e3e6f04a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24, dtype=torch.float32).reshape((2,3,4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ee4821-764b-487e-9af7-531e1d29c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12., 14., 16., 18.],\n",
       "         [20., 22., 24., 26.],\n",
       "         [28., 30., 32., 34.]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=0), X.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fcc2e29-79d8-4b87-9736-ee1f4e0633bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12., 15., 18., 21.],\n",
       "         [48., 51., 54., 57.]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=1), X.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc038f98-bf58-4ec4-9472-11bb06b6864c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6., 22., 38.],\n",
       "         [54., 70., 86.]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=2), X.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63593701-9a56-4913-9620-8808cb14f345",
   "metadata": {},
   "source": [
    "Note: You can also get the sum for higher order axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2f0a0-cc31-47d7-b1e7-5f4f4e6b485a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
